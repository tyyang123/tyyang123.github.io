<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142527943-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-142527943-1');
</script>

  <title>Tianyu Yang</title>
  
  <meta name="author" content="Tianyu Yang">
  <meta name="description" content="Tianyu Yang's personal homepage">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="oM12UU1-G2YRRPoGwO6WTEwnpsqY6Wi_6TKPFBIsiQo" />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianyu Yang</name>
              </p>
              <p>I am a Researcher at International Digital Economy Academy (IDEA). Prior to that, I worked at Tencent AI Lab. I received my PhD from <a href="http://www.cityu.edu.hk/">City University of Hong Kong</a>, advised by <a href="http://www.cs.cityu.edu.hk/~abchan/">Prof. Antoni B. Chan</a>. My research interests span multiple areas, including generative models, multimodal learning, self-supervised learning, and video understanding. Lately, my primary focus has been on generative AI, particularly in the areas of video and 3D content generation and manipulation.
              
              <p style="text-align:center">
                <a href="mailto:tianyu-yang@outlook.com">Email</a> &nbsp/&nbsp
                <!--<a href="TianyuYang-CV.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=BXsWsf8AAAAJ&hl">Google Scholar</a>&nbsp/&nbsp
                <a href="https://github.com/tyyang123">Github</a>
              </p>

              <!--<p style="color:#FF0000";> <b>I am looking for research interns to work on 3D generative models. Drop me an email if you are interested. </b> </p> -->
              
            </td>

            <td style="padding:2.5%;width:20%;max-width:40%">
              <a href="tianyu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="tianyu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
          

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.13221.pdf">
                <papertitle>Latent Video Diffusion Models for High-Fidelity Long Video Generation</papertitle>
              </a>
              <br>
              Yingqing He, <strong>Tianyu Yang</strong>, Yong Zhang, Ying Shan, Qifeng Chen
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2211.13221.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://yingqinghe.github.io/LVDM/">project</a> 
            </td>
         </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="tianyu-yang.com">
                <papertitle>Scalable Video Object Segmentation with Simplified Framework</papertitle>
              </a>
              <br>
              Qiangqiang Wu, <strong>Tianyu Yang</strong>, Wei Wu, Antoni B. Chan
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="tianyu-yang.com">arXiv</a> &nbsp/&nbsp
              <a href="tianyu-yang.com">code</a>
            </td>
         </tr>
          
          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.00571.pdf">
                <papertitle>DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks</papertitle>
              </a>
              <br>
              Qiangqiang Wu, <strong>Tianyu Yang</strong>, Ziquan Liu, Baoyuan Wu, Ying Shan, Antoni B. Chan
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.00571.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/jimmy-dq/dropmae">code</a>
            </td>
         </tr>
          
           <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/locvtp.pdf">
                <papertitle>LocVTP: Video-Text Pre-training for Temporal Localization</papertitle>
              </a>
              <br>
              Meng Cao, <strong>Tianyu Yang</strong>, Junwu Weng, Can Zhang, Jue Wang and Yuexian Zou 
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2207.10362.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/mengcaopku/LocVTP">code</a> &nbsp/&nbsp
              <a href="resources/locvtp-supp.pdf">supp</a>
            </td>
         </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/up-tal.pdf">
                <papertitle>Unsupervised Pre-training for Temporal Action Localization Tasks</papertitle>
              </a>
              <br>
              Can Zhang, <strong>Tianyu Yang</strong>, Junwu Weng, Meng Cao, Jue Wang and Yuexian Zou 
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.13609.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/zhang-can/UP-TAL">code</a> &nbsp/&nbsp
              <a href="resources/up-tal-supp.pdf">supp</a>
            </td>
         </tr>


         <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/dcc.pdf">
                <papertitle>Exploring Denoised Cross-video Contrast for Weakly-supervised Temporal Action Localization</papertitle>
              </a>
              <br>
              Jingjing Li, <strong>Tianyu Yang</strong>, Wei Ji, Jue Wang and Li Cheng 
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://tianyu-yang.com">arXiv</a> &nbsp/&nbsp
              <a href="https://tianyu-yang.com">code</a>
            </td>
         </tr>


         <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/swem.pdf">
                <papertitle>SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization</papertitle>
              </a>
              <br>
              Zhihui Lin, <strong>Tianyu Yang</strong>, Maomao Li, Ziyu Wang, Chun Yuan, Wenhao Jiang, Wei Liu
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2208.10128.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/lmm077/SWEM">code</a> &nbsp/&nbsp
              <a href="resources/swem-supp.pdf">supp</a>
            </td>
         </tr>


         <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/fame.pdf">
                <papertitle>Motion-aware Contrastive Video Representation Learning via Foreground-background Merging</papertitle>
              </a>
              <br>
              Shuangrui Ding, Maomao Li, <strong>Tianyu Yang</strong>, Rui Qian, Haohang Xu, Qingyi Chen, Jue Wang and Hongkai Xiong 
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2109.15130.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/Mark12Ding/FAME">code</a> &nbsp/&nbsp
              <a href="resources/fame-supp.pdf">supp</a> 
            </td>
         </tr>

    
          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.05905.pdf">
                <papertitle>VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples</papertitle>
              </a>
              <br>
              Tian Pan, Yibing Song, <strong>Tianyu Yang</strong>, Wenhao Jiang, and Wei Liu 
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.05905.pdf">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tinapan-pt/VideoMoCo">code</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1907.12006.pdf">
                <papertitle>ROAM: Recurrently Optimizing Tracking Model</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Pengfei Xu, Runbo Hu, Hua Chai, and Antoni B. Chan 
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="https://github.com/tyyang123/ROAM">code</a> &nbsp/&nbsp
              <a href="resources/roam-otb100.zip">otb100-results</a> &nbsp/&nbsp
              <a href="resources/roam-lasot.zip">lasot-results</a>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="resources/memdtc.pdf">
                <papertitle>Visual Tracking via Dynamic Memory Networks</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Antoni B. Chan 
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2019
              <br>
              <a href="https://github.com/tyyang123/MemDTC">code</a> &nbsp/&nbsp
              <a href="resources/pami-otb100-results.zip">otb100-results</a> &nbsp/&nbsp
              <a href="resources/pami-vot2015-results.zip">vot2015-results</a> &nbsp/&nbsp
              <a href="resources/pami-vot2016-results.zip">vot2016-results</a> &nbsp/&nbsp
              <a href="resources/pami-vot2017-results.zip">vot2017-results</a>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1803.07268.pdf">
                  <papertitle>Learning Dynamic Memory Networks for Object Tracking</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Antoni B. Chan 
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018 
              <br>
              <a href="https://github.com/tyyang123/MemTrack">code</a> &nbsp/&nbsp
              <a href="resources/MemTrack-OTB-100.zip">otb100-results</a> &nbsp/&nbsp
              <a href="resources/MemTrack-VOT-2016.zip">vot2016-results</a>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://visal.cs.cityu.edu.hk/static/pubs/journal/pami18-dphem.pdf">
                <papertitle>Density-Preserving Hierarchical EM Algorithm: Simplifying Gaussian Mixture Models for Approximate Inference</papertitle>
              </a>
              <br>
              Lei Yu, <strong>Tianyu Yang</strong>, Antoni B. Chan 
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2018 
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1708.03874.pdf">
                <papertitle>Recurrent Filter Learning for Visual Tracking</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Antoni B. Chan 
              <br>
              <em>Workshop on Visual Object Tracking (VOT) Chanllenge</em>, <strong>ICCV</strong>, 2017 
              <br>
              <a href="https://github.com/tyyang123/RFL">code</a> &nbsp/&nbsp
              <a href="resources/RFL-OTB-100.zip">otb100-results</a> &nbsp/&nbsp
              <a href="resources/RFL-VOT-2016.zip">vot2016-results</a>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://visal.cs.cityu.edu.hk/static/pubs/conf/nips16w-dphem.pdf">
                <papertitle>Approximate Inference for Generic Likelihoods via Density-Preserving GMM Simplification</papertitle>
              </a>
              <br>
              Lei Yu, <strong>Tianyu Yang</strong>, Antoni B. Chan <br>
              <em> Workshop on Advances in Approximate Bayesian Inference</em>, <strong>NeurIPS</strong>, 2016
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740843">
                <papertitle>Robust Object Tracking With Reacquisition Ability Using Online Learned Detector</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Baopu Li, Max Q.-H. Meng<br>
              <em> IEEE transactions on Cybernetics </em>, 2014
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6631254">
                <papertitle>Adaptive Visual Tracking with Reacquisition Ability for Arbitrary Objects</papertitle>
              </a>
              <br>
              <strong>Tianyu Yang</strong>, Baopu Li, Chao Hu, Max Q.-H. Meng<br>
              <em> International Conference on Robotics and Automation (<strong>ICRA</strong>) </em>, 2013
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:10px"><tbody>
          <tr>
            <td style="padding:10px; width:100%;vertical-align:middle">
              <heading>Service</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:10px; padding-top:0px; line-height:0px;vertical-align:middle">
              <p> Senior Program Committee Member for AAAI 2022 </p>
              <br>
              <p> Conference Reviewer for CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI </p>
              <br>
              <p> Journal Reviewer for TPAMI, IJCV, TIP </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="1">
              Template borrowed from <a href="https://jonbarron.info//"> Jon Barron</a>
              </font>
            </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
